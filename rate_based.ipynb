{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/comp-neural-circuits/plasticity-workshop/blob/dev/rate_based.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "70cb080a-e9a3-4007-b9cb-f7ab20b92c2c",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Rate-based Plasticity Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6f485884-d360-4f23-9481-8c834974a155",
    "deepnote_cell_type": "text-cell-h2",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "## Hebbian Plasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9faf8d82-d8b7-41e5-a255-290fb1557024",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Goals**\n",
    "+ Covariance-based learning rule is equivalent to detecting the first principal component of the activity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c9f07e91-b33e-49da-9b42-eb3a744cded3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a92bc37-cb67-4f91-aea7-3682fdd62113",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8501,
    "execution_start": 1644223398431,
    "is_code_hidden": true,
    "is_output_hidden": true,
    "source_hash": "7c291202",
    "tags": [
     "hide_output",
     "remove_output",
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib ipywidgets scikit-learn --quiet\n",
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "#plt.style.use(\"https://github.com/comp-neural-circuits/plasticity-workshop/raw/dev/plots_style.txt\")\n",
    "plt.style.use(\"plots_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e047227c-e297-436b-aa18-f26e2976f0df",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "94b14042-aade-4b82-8b77-2f6e3d9b3b8e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1644222477799,
    "source_hash": "4199937f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ornstein_uhlenbeck(mean,cov,dt,Ttot,dts=1E-2):\n",
    "    \"\"\"\n",
    "    Generates a multi-dimensional Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters :\n",
    "    mean (numpy vector) : desired mean\n",
    "    cov  (matrix)   : covariance matrix (symmetric, positive definite)\n",
    "    dt   (real)     : timestep output\n",
    "    Tot  (real)     : total time\n",
    "    dts = 1E-3 (real) : simulation timestep\n",
    "\n",
    "    Returns :\n",
    "    times (numpy vector)\n",
    "    rates (numpy matrix)  :  rates[i,j] is the rate of unit i at time times[j]\n",
    "    \"\"\"\n",
    "    times = np.arange(0.0,Ttot,dt)\n",
    "    n = len(mean)\n",
    "    nTs = int(Ttot/dts)\n",
    "    rates_all = np.empty((n,nTs))\n",
    "    rates_all[:,0] = mean\n",
    "    L = lin.cholesky(cov)\n",
    "    nskip = int(dt/dts)\n",
    "    assert round(dts*nskip,5) == dt , \"dt must be multiple of  \" + str(dts)\n",
    "    for t in range(1,nTs):\n",
    "        dr = dts*(mean-rates_all[:,t-1])\n",
    "        dpsi = np.sqrt(2*dts)*(L.T @ rng.standard_normal(n))\n",
    "        rates_all[:,t] = rates_all[:,t-1] + dr + dpsi\n",
    "    # subsample \n",
    "    rates = rates_all[:,::nskip]\n",
    "    return times,rates\n",
    "  \n",
    "def twodimensional_OU(mean1,var1,mean2,var2,corr,dt,Ttot,dts=1E-2):\n",
    "    \"\"\"\n",
    "    Generates samples from a 2D Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters :\n",
    "    mean1 (real) : mean on first dimension\n",
    "    var1  (real) : variance on first dimension (at dt=1. intervals)\n",
    "    mean2 (real) : - \n",
    "    var2  (real) : - \n",
    "    corr  (real) : correlation coefficient \n",
    "    dt   (real)     : timestep output\n",
    "    Tot  (real)     : total time\n",
    "    dts = 1E-3 (real) : simulation timestep\n",
    "\n",
    "    Returns :\n",
    "    times  (numpy vector)\n",
    "    rates1 (numpy vector)\n",
    "    rates2 (numpy vector)\n",
    "    \"\"\"\n",
    "    assert -1<corr<1, \"correlation must be in (-1,1) interval\"\n",
    "    var12 = corr*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    (times, rates) = ornstein_uhlenbeck(\n",
    "      np.array([mean1,mean2]),\n",
    "      cov_mat,\n",
    "      dt,Ttot,dts)\n",
    "    return times, rates[0,:],rates[1,:]\n",
    "\n",
    "\n",
    "def plot_r1_and_r2(correlation=0.0,mean_r1=0.0,mean_r2=0.0,var_r1=1.0,var_r2=1.0):\n",
    "    times,rates1,rates2 = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,0.1,60.0)\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,10)) #gridspec_kw={'height_ratios': [3, 1]})\n",
    "    ax1.plot(times,rates1)\n",
    "    ax1.plot(times,rates2)\n",
    "    ax1.set_xlabel(\"time (s)\")\n",
    "    ax1.set_ylabel(\"rate (Hz)\")\n",
    "    ax1.set_title(\"time traces\")\n",
    "    ax2.scatter(rates1,rates2,color=\"black\")\n",
    "    ax2.set_title(\"samples r1 Vs r2\")\n",
    "    ax2.set_xlabel(\"rate 1 (Hz)\")\n",
    "    ax2.set_ylabel(\"rate 2 (Hz)\")\n",
    "    ax2.axis(\"equal\")\n",
    "    return \n",
    "\n",
    "def analytic_correlation_based(r_means,r_cov,times,weights_start,gamma):\n",
    "    \"\"\"\n",
    "    Computes the analytic solution for the correlation-based\n",
    "    plasticity rule. Assuming the input is a multi dimensional \n",
    "    O-U process.\n",
    "    \n",
    "    Parameters :\n",
    "    r_means (vector) : input means\n",
    "    r_cov   (matrix) : input covariance matrix\n",
    "    times   (vector) : time vector\n",
    "    weights_start (vector) : initial conditions for weights\n",
    "    gamma   (number) : learning coefficient\n",
    "    \n",
    "    Returns :\n",
    "    weights (matrix) : weights[i,k] is the weight from neuron i at time times[k]\n",
    "    \"\"\"\n",
    "    Ntimes = len(times)\n",
    "    N = len(r_means)\n",
    "    weights = np.empty((N,Ntimes))\n",
    "    M = gamma*(r_cov + np.outer(r_means,r_means)) + np.identity(N)\n",
    "    for k in range(Ntimes):\n",
    "        weights[:,k] = np.linalg.matrix_power(M,k) @ weights_start\n",
    "    return weights\n",
    "\n",
    "def analytic_covariance_based(r_means,r_cov,times,weights_start,gamma):\n",
    "    \"\"\"\n",
    "    Computes the analytic solution for the covariance-based\n",
    "    plasticity rule. Assuming the input is a multi dimensional \n",
    "    O-U process.\n",
    "    \n",
    "    Parameters :\n",
    "    r_means (vector) : input means\n",
    "    r_cov   (matrix) : input covariance matrix\n",
    "    times   (vector) : time vector\n",
    "    weights_start (vector) : initial conditions for weights\n",
    "    gamma   (number) : learning coefficient \n",
    "    \n",
    "    Returns :\n",
    "    weights (matrix) : weights[i,k] is the weight from neuron i at time times[k]\n",
    "    \"\"\"\n",
    "    Ntimes = len(times)\n",
    "    N = len(r_means)\n",
    "    weights = np.empty((N,Ntimes))\n",
    "    M = gamma*r_cov + np.identity(N)\n",
    "    for k in range(Ntimes):\n",
    "        weights[:,k] = np.linalg.matrix_power(M,k) @ weights_start\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_weight_update_correlation():\n",
    "    # numeric result\n",
    "    mean_r1,var1 = 3.0,0.3\n",
    "    mean_r2,var2 = 5.0, 0.2\n",
    "    correlation = 0.7\n",
    "    T = 100.0\n",
    "    gamma = 1.0\n",
    "    weights = rng.random(2) + 5.0\n",
    "    times,rates1,rates2 = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,0.2,T)\n",
    "    rates_input = np.array([rates1,rates2])\n",
    "    weight_update = weight_update_correlation(rates_input,weights,gamma)\n",
    "    \n",
    "    # analytic result\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_correlation_based(r_means,cov_mat,np.array([0,T]),weights,gamma)[:,1]\n",
    "    weight_update_an = weights_an - weights\n",
    "    print(f\"expected (approx): {weight_update_an} \\t function output {weight_update}\") \n",
    "    if all(np.isclose(weight_update,weight_update_an,rtol=0.3)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    " \n",
    "def test_weight_update_covariance():\n",
    "    # numeric result\n",
    "    mean_r1,var1 = 3.0,0.3\n",
    "    mean_r2,var2 = 5.0, 0.2\n",
    "    correlation = 0.7\n",
    "    T = 100.0\n",
    "    gamma = 1.0\n",
    "    weights = rng.random(2) + 5.0\n",
    "    times,rates1,rates2 = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,0.2,T)\n",
    "    rates_input = np.array([rates1,rates2])\n",
    "    weight_update = weight_update_covariance(rates_input,weights,gamma)\n",
    "    \n",
    "    # analytic result\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_covariance_based(r_means,cov_mat,np.array([0,T]),weights,gamma)[:,1]\n",
    "    weight_update_an = weights_an - weights\n",
    "    print(f\"expected (approx): {weight_update_an} \\t function output {weight_update}\") \n",
    "    if all(np.isclose(weight_update,weight_update_an,rtol=0.3)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "\n",
    "        \n",
    "        \n",
    "def weight_evolution_correlation(mean_r1,var1,mean_r2,var2,correlation,\n",
    "                                 weights_start,Tstep=100.0,Nsteps=20,gammahat=1E-3,dtsample=0.2):\n",
    "    weight_ret = np.empty(Nsteps)\n",
    "    times_ret = np.arange(0.0,Tstep*Nsteps,Tstep)\n",
    "    gamma = gammahat*Tstep\n",
    "    weights_temp = np.copy(weights_start)\n",
    "    for k in range(Nsteps):\n",
    "        weight_ret[k]=weights_temp[0]\n",
    "        times,rates1,rates2 = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,dtsample,Tstep)\n",
    "        rates_input = np.array([rates1,rates2])\n",
    "        weight_updates = weight_update_correlation(rates_input,weights_temp,gamma)\n",
    "        weights_temp += weight_updates\n",
    "    \n",
    "    # analytic solution\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_correlation_based(r_means,cov_mat,times_ret,weights_start,gamma)\n",
    "        \n",
    "    return times_ret,weight_ret,weights_an[0,:]\n",
    "\n",
    "def weight_evolution_covariance(mean_r1,var1,mean_r2,var2,correlation,\n",
    "                                weights_start,Tstep=100.0,Nsteps=20,gammahat=1E-3,dtsample=0.2):\n",
    "    weight_ret = np.empty(Nsteps)\n",
    "    times_ret = np.arange(0.0,Tstep*Nsteps,Tstep)\n",
    "    gamma = gammahat*Tstep\n",
    "    weights_temp = np.copy(weights_start)\n",
    "    for k in range(Nsteps):\n",
    "        weight_ret[k]=weights_temp[0]\n",
    "        times,rates1,rates2 = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,dtsample,Tstep)\n",
    "        rates_input = np.array([rates1,rates2])\n",
    "        weight_updates = weight_update_covariance(rates_input,weights_temp,gamma)\n",
    "        weights_temp += weight_updates\n",
    "    \n",
    "    # analytic solution\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_covariance_based(r_means,cov_mat,times_ret,weights_start,gamma)\n",
    "        \n",
    "    return times_ret,weight_ret,weights_an[0,:]\n",
    "\n",
    "def weight_evo_all(mean_r1,var1,mean_r2,var2,correlation,):\n",
    "    weights_start = np.array([1E-2,1E-2])\n",
    "    times,wcorr,wcorran = weight_evolution_correlation(mean_r1,var1,mean_r2,\\\n",
    "                                                       var2,correlation,weights_start)\n",
    "    _,wcov,wcovan = weight_evolution_covariance(mean_r1,var1,mean_r2,\\\n",
    "                                                       var2,correlation,weights_start)\n",
    "    times += times[1]-times[0] # time starts from value >0 , so I can plot in log-log scale\n",
    "    fig,ax = plt.subplots()\n",
    "    linecorr, = ax.plot(times,wcorr,color=\"xkcd:ocean blue\",label=\"correlation-based\")\n",
    "    # plt.plot(times,wcorran,'--',color=\"xkcd:ocean blue\",alpha=0.8)\n",
    "    \n",
    "    linecov, = ax.plot(times,wcov,color=\"xkcd:blood red\",label=\"covariance-based\")\n",
    "    # plt.plot(times,wcovan,'--',color=\"xkcd:blood red\",alpha=0.8)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('time (s)')\n",
    "    ax.set_ylabel('synaptic weight')\n",
    "    ax.legend(handles=[linecorr,linecov])\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize noisy rate inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we generate a noisy rate trace from $N$ neurons. $r_j(t)$ indicates the rate of neuron $j=1,2,\\ldots\\,N$ at time $t$. Neurons are, in general, correlated with each other.\n",
    "\n",
    "In the figure below, you can see the traces of 2 neurons, simulated for 60 seconds. You don't need to read or understand this code. Try to modify some of the parameters to understand their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_r1_and_r2,correlation=(-0.99,0.99,0.01) , mean_r1=(0.0,5.0,0.1),mean_r2=(0.0,5.0,0.1),\n",
    "        var_r1=(0.01,2.0,0.01), var_r2=(0.01,2.0,0.01));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have extra time, check the documentation of the functions `ornstein_uhlenbeck(...)` and `twodimensional_OU(...)`, which are used to generate these traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute response of output neuron based on input activity and weights  (exercise 1 ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r_\\text{out}(t) = \\sum_{j=1}^N w_j r_j(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_response(r_input,weights):\n",
    "    \"\"\"\n",
    "    Computes the response of a neuron that receives a series of inputs over time.  \n",
    "    \n",
    "    Parameters :\n",
    "    r_input (matrix) :  r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (vector) :  weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    \n",
    "    Returns :\n",
    "    r_output (vector) : r_output[t] is the rate of the output neuron at timestep t\n",
    "    \"\"\"\n",
    "    \n",
    "    # I think this can be done with np.dot , but I don't like to see it applied to matrices\n",
    "    # so I propose a more canonical broadcasting\n",
    "    \n",
    "    r_input_weighted = r_input * weights[:,np.newaxis] # multiply columnwise\n",
    "    r_output = r_input_weighted.sum(axis=0) # and sum columnwise\n",
    "    r_output[r_output < 0 ] = 0  # avoid negative rates\n",
    "    return r_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute weight update for correlation based and covariance based rule\n",
    "\n",
    "Now that you have the reponse of the output neuron  $r_\\text{out}(t)$ , you can calculate the update in synaptic weights due to rate-based plasticiy.\n",
    "\n",
    "#### Some notation\n",
    "We use $\\left< \\; \\ldots \\; \\right>_t$ to indicate an average over time. For mean rates, we further simplify the notation, taking the form $ \\bar{r}_j$. Therefore:\n",
    "$$\n",
    "\\bar{r}_j = \\left< r_j(t) \\right>_t = \n",
    "\\frac1T \\int_0^T r_j(t) \\;\\mathrm d t = \n",
    "\\frac{1}{N_T} \\sum_k  r_j(t_k)\n",
    "$$\n",
    "The last equality represents the fact that $r(t)$ is discretized in our code, and $N_T$ indicates the number of discretized steps.\n",
    "\n",
    "\n",
    "#### Correlation-based rule\n",
    "The correlation-based rule is defined as :\n",
    "$$ \n",
    "\\Delta w_j = \\gamma \\; \\left<  r_\\text{out}(t) \\; r_j(t)  \\right>_t\n",
    "$$\n",
    "\n",
    "The second moment can be computed numerically simply as the mean of the element-wise product between the two time series.\n",
    "$$\n",
    "\\left<  r_\\text{out}(t) \\; r_j(t)  \\right>_t = \\frac{1}{N_T}\\sum_k  r_\\text{out}(t_k) \\; r_j(t_k) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_correlation(r_input,weights,gamma):\n",
    "    \"\"\"\n",
    "    Computes the weight updates according to the correlation rule\n",
    "    \n",
    "    Parameters :\n",
    "    r_input (matrix) : r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (vector) : weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    gamma   (number) : plasticity parameter\n",
    "    T       (number) : total simulation time, in seconds\n",
    "    \n",
    "    Returns :\n",
    "    weight_updates (vector) : the update on each weight after this training interval\n",
    "    \"\"\"\n",
    "\n",
    "    # TIPS : \n",
    "    # use the rate_response function that you defined before !\n",
    "   \n",
    "    r_output = rate_response(r_input,weights)\n",
    "    r_product  = r_output * r_input  # broadcast by row\n",
    "    weight_updates = gamma * r_product.mean(axis=1) # average over time dimension\n",
    "    return weight_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify your function !\n",
    "\n",
    "To test your function, run the cell below. It should return `test PASSED !` if it errors, or returns `test FAILED !` please review your solution before you proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "test_weight_update_correlation()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance based rule\n",
    "In the covariance-based rule, we are using a covariance, instead :\n",
    "\n",
    "$$ \n",
    "\\Delta w_j = \\gamma \\; \\left<  \\left(r_\\text{out}(t) - \\bar{r}_\\text{out}\\right)\n",
    "\\; \\left(r_j(t) - \\bar{r}_j \\right)  \\right>_t \\quad \\text{with} \\quad \n",
    "\\bar{r}_\\text{out} = \\left< r_\\text{out}(t) \\right>_t \\quad \\text{and} \\quad\n",
    "\\bar{r}_j = \\left< r_j(t) \\right>_t \n",
    "$$\n",
    "\n",
    "To compute this quantity numerically, simply subtract the means from the rate traces, and then proceed as in the previous case.\n",
    "\n",
    "\n",
    "----\n",
    "*Technical note:*  \n",
    "the weight update rate should really be $\\gamma=\\hat{\\gamma}\\;T$, where $\\hat{\\gamma}$ is the weight update *per second*.  \n",
    "Consider also dimensional analysis: if $\\text{rate}\\sim \\text{time}^{-1}$, then it must be $\\gamma \\sim \\text{time}\\times \\text{weight}$.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_covariance(r_input,weights,gamma):\n",
    "    \"\"\"\n",
    "    Computes the weight updates according to the covariance rule\n",
    "    \n",
    "    Parameters :\n",
    "    r_input (matrix) :  r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (vector) :  weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    \n",
    "    Returns :\n",
    "    weight_updates (vector) : the update on each weight after this training interval\n",
    "    \"\"\"\n",
    "    \n",
    "    # TIPS : \n",
    "    # it is very similar to the correlation rule, except you need to subtract the mean rates!\n",
    "    # r_input_means = r_input.mean(axis=1)  # (mean over time axis)\n",
    "    r_output = rate_response(r_input,weights)\n",
    "    r_output_mean = r_output.mean() # mean of a vector -> scalar value\n",
    "    r_output_meanzero = r_output - r_output_mean\n",
    "    r_input_means = r_input.mean(axis=1) # mean over time axis\n",
    "    r_input_meanzero = r_input - r_input_means[:,np.newaxis] # broadcast on columns\n",
    "    # now same as before\n",
    "    r_product = r_output_meanzero * r_input_meanzero # elementwise product\n",
    "    weight_updates = gamma * r_product.mean(axis=1) # mean over time\n",
    "    return weight_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify your function !\n",
    "\n",
    "As before, to test your function, run the cell below. It should return `test PASSED !` if it errors, or returns `test FAILED !` please review your solution before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "test_weight_update_covariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize $w(t)$ for covariance vs correlation rule\n",
    "\n",
    "Here we show how how one synaptic weight would change over time, according to the two rules. This system has two input neurons and one output neuron. For simplicity we only show the first weights.\n",
    "\n",
    "The code simulates rate activity for $t=100\\; \\text{s}$, keeping weights stationary, and then updates the weights based on the correlation/covariance computed in that interval. This is repeated several times so that we can observe the change of weight over time.\n",
    "\n",
    "The weight is not stable, and tends to reach very high values, therefore we show it using a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(weight_evo_all, mean_r1=(0.0,5.0,0.1),var1=(0.01,2.0,0.01),\n",
    "         mean_r2=(0.0,5.0,0.1),var2=(0.01,2.0,0.01) , correlation=(-0.99,0.99,0.01));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Which rule results in a faster growth of the weight, and why ?\n",
    "+ Which factor is determining the higher growth ?\n",
    "+ When do the two rules coincide, and why is that ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5e3b54db-e48f-43a7-b40f-09467f2d2c99",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
