{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/comp-neural-circuits/plasticity-workshop/blob/dev/rate_based.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "70cb080a-e9a3-4007-b9cb-f7ab20b92c2c",
    "deepnote_cell_type": "text-cell-h1",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Hebbian Plasticity (Rate-Based)\n",
    "## Goals\n",
    "\n",
    "**TODO**\n",
    "\n",
    "+ Covariance-based learning rule is equivalent to detecting the first principal component of the activity\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a92bc37-cb67-4f91-aea7-3682fdd62113",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8501,
    "execution_start": 1644223398431,
    "is_code_hidden": true,
    "is_output_hidden": true,
    "source_hash": "7c291202",
    "tags": [
     "hide_output",
     "remove_output",
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib ipywidgets scikit-learn panel --quiet\n",
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# for the PCA\n",
    "from sklearn.decomposition import PCA\n",
    "plt.style.use(\"https://github.com/comp-neural-circuits/plasticity-workshop/raw/dev/plots_style.txt\")#\n",
    "#plt.style.use(\"plots_style.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e047227c-e297-436b-aa18-f26e2976f0df",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "94b14042-aade-4b82-8b77-2f6e3d9b3b8e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1644222477799,
    "source_hash": "4199937f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ornstein_uhlenbeck(mean,cov,dt,Ttot,dts=1E-2):\n",
    "    \"\"\"\n",
    "    Generates a multi-dimensional Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters :\n",
    "    mean (1D numpy array) : desired mean\n",
    "    cov  (2D numpy array)   : covariance matrix (symmetric, positive definite)\n",
    "    dt   (number)     : timestep output\n",
    "    Tot  (number)     : total time\n",
    "    dts = 1E-3 (number) : simulation timestep\n",
    "\n",
    "    Returns :\n",
    "    times (1D numpy array)\n",
    "    rates (2D numpy array)  :  rates[i,j] is the rate of unit i at time times[j]\n",
    "    \"\"\"\n",
    "    times = np.arange(0.0,Ttot,dt)\n",
    "    n = len(mean)\n",
    "    nTs = int(Ttot/dts)\n",
    "    rates_all = np.empty((n,nTs))\n",
    "    rates_all[:,0] = mean\n",
    "    L = lin.cholesky(cov)\n",
    "    nskip = int(dt/dts)\n",
    "    assert round(dts*nskip,5) == dt , \"dt must be multiple of  \" + str(dts)\n",
    "    for t in range(1,nTs):\n",
    "        dr = dts*(mean-rates_all[:,t-1])\n",
    "        dpsi = np.sqrt(2*dts)*(L.T @ rng.standard_normal(n))\n",
    "        rates_all[:,t] = rates_all[:,t-1] + dr + dpsi\n",
    "    # subsample \n",
    "    rates = rates_all[:,::nskip]\n",
    "    return times,rates\n",
    "  \n",
    "def twodimensional_OU(mean1,var1,mean2,var2,corr,dt,Ttot,dts=1E-2):\n",
    "    \"\"\"\n",
    "    Generates samples from a 2D Ornstein-Uhlenbeck process.\n",
    "\n",
    "    Parameters :\n",
    "    mean1 (real) : mean on first dimension\n",
    "    var1  (real) : variance on first dimension (at dt=1. intervals)\n",
    "    mean2 (real) : - \n",
    "    var2  (real) : - \n",
    "    corr  (real) : correlation coefficient \n",
    "    dt   (real)     : timestep output\n",
    "    Tot  (real)     : total time\n",
    "    dts = 1E-3 (real) : simulation timestep\n",
    "\n",
    "    Returns :\n",
    "    times  (1D numpy array)\n",
    "    rates1 (1D numpy array)\n",
    "    rates2 (1D numpy array)\n",
    "    \"\"\"\n",
    "    assert -1<corr<1, \"correlation must be in (-1,1) interval\"\n",
    "    var12 = corr*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    (times, rates) = ornstein_uhlenbeck(\n",
    "      np.array([mean1,mean2]),\n",
    "      cov_mat,\n",
    "      dt,Ttot,dts)\n",
    "    return times, rates #rates[0,:],rates[1,:]\n",
    "\n",
    "def test_rate_response():\n",
    "    r_input = np.array( [ [1.2, 4.44, 234.0] ,[2, 54.04, 4.03], [6.2, 2.9, 0.888]] )\n",
    "    weights = np.array([0.789, -3.4, 4.0])\n",
    "    \n",
    "    \n",
    "    res1 = rate_response(r_input,weights)\n",
    "    res1_expected = np.array([ np.dot(r_input[:,0],weights), \n",
    "                               np.dot(r_input[:,1],weights),\n",
    "                               np.dot(r_input[:,2],weights)])\n",
    "    print(\"Testing rate response\")\n",
    "    print(f\"expected: {res1_expected} \\t function output {res1}\") \n",
    "    if all(np.isclose(res1,res1_expected,rtol=0.001)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "    return \n",
    "\n",
    "def plot_r1_and_r2(correlation=0.0,mean_r1=0.0,mean_r2=0.0,var_r1=1.0,var_r2=1.0):\n",
    "    times,rates = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,0.1,60.0)\n",
    "    rates1 = rates[0,:]\n",
    "    rates2 = rates[1,:]\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,10)) #gridspec_kw={'height_ratios': [3, 1]})\n",
    "    ax1.plot(times,rates1)\n",
    "    ax1.plot(times,rates2)\n",
    "    ax1.set_xlabel(\"time (s)\")\n",
    "    ax1.set_ylabel(\"rate (Hz)\")\n",
    "    ax1.set_title(\"time traces\")\n",
    "    ax2.scatter(rates1,rates2,color=\"black\")\n",
    "    ax2.set_title(\"samples r1 Vs r2\")\n",
    "    ax2.set_xlabel(\"rate 1 (Hz)\")\n",
    "    ax2.set_ylabel(\"rate 2 (Hz)\")\n",
    "    ax2.axis(\"equal\")\n",
    "    return \n",
    "\n",
    "def analytic_correlation_based(r_means,r_cov,times,weights_start,gamma):\n",
    "    \"\"\"\n",
    "    Computes the analytic solution for the correlation-based\n",
    "    plasticity rule. Assuming the input is a multi dimensional \n",
    "    O-U process.\n",
    "    \n",
    "    Parameters :\n",
    "    r_means (1D numpy array) : mean rate for each input neuron\n",
    "    r_cov   (2D numpy array) : covariance matrix of input activity\n",
    "    times   (1D numpy array) : time vector\n",
    "    weights_start (1D numpy array) : initial conditions for weights\n",
    "    gamma   (number) : learning coefficient\n",
    "    \n",
    "    Returns :\n",
    "    weights (2D numpy array) : weights[i,k] is the weight from neuron i at time times[k]\n",
    "    \"\"\"\n",
    "    Ntimes = len(times)\n",
    "    N = len(r_means)\n",
    "    weights = np.empty((N,Ntimes))\n",
    "    M = gamma*(r_cov + np.outer(r_means,r_means)) + np.identity(N)\n",
    "    for k in range(Ntimes):\n",
    "        weights[:,k] = np.linalg.matrix_power(M,k) @ weights_start\n",
    "    return weights\n",
    "\n",
    "def analytic_covariance_based(r_means,r_cov,times,weights_start,gamma):\n",
    "    \"\"\"\n",
    "    Computes the analytic solution for the covariance-based\n",
    "    plasticity rule. Assuming the input is a multi dimensional \n",
    "    O-U process.\n",
    "    \n",
    "    Parameters :\n",
    "    r_means (1D numpy array) : input means\n",
    "    r_cov   (2D numpy array) : input covariance matrix\n",
    "    times   (1D numpy array) : time vector\n",
    "    weights_start (1D numpy array) : initial conditions for weights\n",
    "    gamma   (number) : learning coefficient \n",
    "    \n",
    "    Returns :\n",
    "    weights (2D numpy array) : weights[i,k] is the weight from neuron i at time times[k]\n",
    "    \"\"\"\n",
    "    Ntimes = len(times)\n",
    "    N = len(r_means)\n",
    "    weights = np.empty((N,Ntimes))\n",
    "    M = gamma*r_cov + np.identity(N)\n",
    "    for k in range(Ntimes):\n",
    "        weights[:,k] = np.linalg.matrix_power(M,k) @ weights_start\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "def test_weight_update_correlation():\n",
    "    # numeric result\n",
    "    mean_r1,var1 = 3.0,0.3\n",
    "    mean_r2,var2 = 5.0, 0.2\n",
    "    correlation = 0.7\n",
    "    T = 100.0\n",
    "    gamma = 1.0\n",
    "    weights = rng.random(2) + 5.0\n",
    "    times,rates_input = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,0.2,T)\n",
    "    weight_update = weight_update_correlation(rates_input,weights,gamma)\n",
    "    \n",
    "    # analytic result\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_correlation_based(r_means,cov_mat,np.array([0,T]),weights,gamma)[:,1]\n",
    "    weight_update_an = weights_an - weights\n",
    "    print(f\"expected (approx): {weight_update_an} \\t function output {weight_update}\") \n",
    "    if all(np.isclose(weight_update,weight_update_an,rtol=0.3)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "    return\n",
    " \n",
    "def test_weight_update_covariance():\n",
    "    # numeric result\n",
    "    mean_r1,var1 = 3.0,0.3\n",
    "    mean_r2,var2 = 5.0, 0.2\n",
    "    correlation = 0.7\n",
    "    T = 100.0\n",
    "    gamma = 1.0\n",
    "    weights = rng.random(2) + 5.0\n",
    "    times,rates_input = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,0.2,T)\n",
    "    weight_update = weight_update_covariance(rates_input,weights,gamma)\n",
    "    \n",
    "    # analytic result\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_covariance_based(r_means,cov_mat,np.array([0,T]),weights,gamma)[:,1]\n",
    "    weight_update_an = weights_an - weights\n",
    "    print(f\"expected (approx): {weight_update_an} \\t function output {weight_update}\") \n",
    "    if all(np.isclose(weight_update,weight_update_an,rtol=0.3)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "\n",
    "        \n",
    "def weight_evolution_correlation(mean_r1,var1,mean_r2,var2,correlation,\n",
    "                                 weights_start,Tstep=60.0,Nsteps=30,gammahat=1E-3,dtsample=0.2):\n",
    "    weight_ret = np.empty(Nsteps)\n",
    "    times_ret = np.arange(0.0,Tstep*Nsteps,Tstep)\n",
    "    gamma = gammahat*Tstep\n",
    "    weights_temp = np.copy(weights_start)\n",
    "    for k in range(Nsteps):\n",
    "        # save weight\n",
    "        weight_ret[k]=weights_temp[0]\n",
    "        # simulate input neuron activity for Tstep duration\n",
    "        _times,_rates_input = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,dtsample,Tstep)\n",
    "        # compute weight update value, and update the weights\n",
    "        _weight_updates = weight_update_correlation(_rates_input,weights_temp,gamma)\n",
    "        weights_temp += _weight_updates\n",
    "    \n",
    "    # analytic solution\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_correlation_based(r_means,cov_mat,times_ret,weights_start,gamma)\n",
    "        \n",
    "    return times_ret,weight_ret,weights_an[0,:]\n",
    "\n",
    "def weight_evolution_covariance(mean_r1,var1,mean_r2,var2,correlation,\n",
    "                                weights_start,Tstep=60.0,Nsteps=30,gammahat=1E-3,dtsample=0.2):\n",
    "    weight_ret = np.empty(Nsteps)\n",
    "    times_ret = np.arange(0.0,Tstep*Nsteps,Tstep)\n",
    "    gamma = gammahat*Tstep\n",
    "    weights_temp = np.copy(weights_start)\n",
    "    for k in range(Nsteps):\n",
    "        weight_ret[k]=weights_temp[0]\n",
    "        _times,_rates_input = twodimensional_OU(mean_r1,var1,mean_r2,var2,correlation,dtsample,Tstep)\n",
    "        weight_updates = weight_update_covariance(_rates_input,weights_temp,gamma)\n",
    "        weights_temp += weight_updates\n",
    "    \n",
    "    # analytic solution\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    r_means = np.array([mean_r1,mean_r2])\n",
    "    weights_an = analytic_covariance_based(r_means,cov_mat,times_ret,weights_start,gamma)\n",
    "        \n",
    "    return times_ret,weight_ret,weights_an[0,:]\n",
    "\n",
    "def weight_evo_all(mean_r1,var1,mean_r2,var2,correlation,):\n",
    "    # start with small initial weights\n",
    "    weights_start = np.array([1E-2,1E-2])\n",
    "    \n",
    "    # computes values for corelation-based rule\n",
    "    times,wcorr,wcorran = weight_evolution_correlation(mean_r1,var1,mean_r2,\\\n",
    "                                                       var2,correlation,weights_start)\n",
    "    # computes values for covariance-based rule\n",
    "    _,wcov,wcovan = weight_evolution_covariance(mean_r1,var1,mean_r2,\\\n",
    "                                                       var2,correlation,weights_start)\n",
    "    # plotting\n",
    "    times += times[1]-times[0] # time starts from value >0 , so I can plot in log-log scale\n",
    "    fig,ax = plt.subplots()\n",
    "    linecorr, = ax.plot(times,wcorr,color=\"xkcd:ocean blue\",label=\"correlation-based\")\n",
    "    # plt.plot(times,wcorran,'--',color=\"xkcd:ocean blue\",alpha=0.8)\n",
    "    \n",
    "    linecov, = ax.plot(times,wcov,color=\"xkcd:blood red\",label=\"covariance-based\")\n",
    "    # plt.plot(times,wcovan,'--',color=\"xkcd:blood red\",alpha=0.8)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('time (s)')\n",
    "    ax.set_ylabel('synaptic weight')\n",
    "    ax.legend(handles=[linecorr,linecov])\n",
    "    return \n",
    "\n",
    "#### PCA PART ###\n",
    "\n",
    "def first_principal_component(x):\n",
    "    \"\"\"\n",
    "    Computes the first principal component from x\n",
    "    \n",
    "    Parameters:\n",
    "    x (2D numpy array) :  x[k,i] is the value of the i component of x at time t[k]\n",
    "    \n",
    "    Returns :\n",
    "    pca1 (1D numpy array) : vector of norm 1 that captures the direction of maximum variability\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(x)\n",
    "    return pca.components_[0,:]\n",
    "\n",
    "\n",
    "def plot_r1_and_r2_with_PCA(mean_r1=0.0,mean_r2=0.0,var_r1=1.0,var_r2=1.0,correlation=0.6,):\n",
    "    times,rates_input = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,0.1,60.0)\n",
    "    rates1 = rates_input[0,:]\n",
    "    rates2 = rates_input[1,:]\n",
    "    pcavec = first_principal_component(rates_input.T)\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,10)) #gridspec_kw={'height_ratios': [3, 1]})\n",
    "    ax1.plot(times,rates1)\n",
    "    ax1.plot(times,rates2)\n",
    "    ax1.set_xlabel(\"time (s)\")\n",
    "    ax1.set_ylabel(\"rate (Hz)\")\n",
    "    ax1.set_title(\"time traces\")\n",
    "    \n",
    "    ax2.scatter(rates1,rates2,color=\"black\")\n",
    "    ax2.set_title(\"samples r1 Vs r2\")\n",
    "    ax2.set_xlabel(\"rate 1 (Hz)\")\n",
    "    ax2.set_ylabel(\"rate 2 (Hz)\")\n",
    "    ax2.axis(\"equal\")\n",
    "    xmin,xmax = ax2.get_xlim()\n",
    "    ymin,ymax = ax2.get_ylim()\n",
    "    xpca_plot = np.array([-pcavec[0]*100,pcavec[0]*100])+mean_r1\n",
    "    ypca_plot = np.array([-pcavec[1]*100,pcavec[1]*100])+mean_r2\n",
    "    ax2.plot(xpca_plot,ypca_plot,\"--\",color=\"red\")\n",
    "    ax2.set_xlim((xmin,xmax))\n",
    "    ax2.set_ylim((ymin,ymax))\n",
    "    return\n",
    "\n",
    "def plot_covrule_and_PCA(mean_r1=0.0,mean_r2=0.0,var_r1=1.0,var_r2=1.0,correlation=0.6):\n",
    "    gammahat=1E-3\n",
    "    Tstep=100.\n",
    "    Nsteps=20\n",
    "    dtsample=1.0\n",
    "    # simulate activity once\n",
    "    # compute first pricincipal component\n",
    "    times,rates_input = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,0.2,100.0)\n",
    "    rates1 = rates_input[0,:]\n",
    "    rates2 = rates_input[1,:]\n",
    "    pcavec = first_principal_component(rates_input.T)\n",
    "    \n",
    "    # now, apply covariance rule with repeated iterations\n",
    "    weights_start = np.array([1E-1,1E-1])\n",
    "    gamma = gammahat*Tstep\n",
    "    weights_end = np.copy(weights_start)\n",
    "    for k in range(Nsteps):\n",
    "        _times,_rates_input = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,dtsample,Tstep)\n",
    "        _weight_updates = weight_update_covariance(_rates_input,weights_end,gamma)\n",
    "        weights_end += _weight_updates\n",
    "    \n",
    "    # normalize the weights \n",
    "    weights_end = weights_end / lin.norm(weights_end)\n",
    "    \n",
    "    # do the plot\n",
    "    fig,ax= plt.subplots() \n",
    "    \n",
    "    ax.scatter(rates1,rates2,color=\"black\")\n",
    "    ax.set_xlabel(\"rate 1 (Hz)\")\n",
    "    ax.set_ylabel(\"rate 2 (Hz)\")\n",
    "    ax.axis(\"equal\")\n",
    "    xmin,xmax = ax.get_xlim()\n",
    "    ymin,ymax = ax.get_ylim()\n",
    "    xpca_plot = np.array([-pcavec[0]*100,pcavec[0]*100])+mean_r1\n",
    "    ypca_plot = np.array([-pcavec[1]*100,pcavec[1]*100])+mean_r2\n",
    "    linepca,=ax.plot(xpca_plot,ypca_plot,\"--\",color=\"red\",label=\"1st principal component \")\n",
    "    \n",
    "    xw_plot = np.array([-weights_end[0]*100,weights_end[0]*100])+mean_r1\n",
    "    yw_plot = np.array([-weights_end[1]*100,weights_end[1]*100])+mean_r2\n",
    "    lineweights,=ax.plot(xw_plot,yw_plot,\"--\",color=\"blue\",label=\"weights after learning\")\n",
    "    \n",
    "    ax.set_xlim((xmin,xmax))\n",
    "    ax.set_ylim((ymin,ymax))\n",
    "    \n",
    "    ax.legend(handles=[linepca,lineweights])\n",
    "    return\n",
    "\n",
    "\n",
    "def test_weight_update_bounded():\n",
    "    v1,v2,c,gamma,w1,w2 = 2.,1.23,0.6,1E-2,23.,7.8\n",
    "    v12 = c*np.sqrt(v1*v2)\n",
    "    cov_mat = np.array([[v1,v12],[v12,v2]])\n",
    "    weights = np.array([w1,w2])\n",
    "    res1 = weight_update_bounded_subtractive(cov_mat,weights,gamma)\n",
    "    res1_expected = np.array([  0.11050919, -0.11050919 ])\n",
    "    print(\"Testing subtractive normalization\")\n",
    "    print(f\"expected: {res1_expected} \\t function output {res1}\") \n",
    "    if all(np.isclose(res1,res1_expected,rtol=0.001)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "    print(\"\\nTesting divisive normalization\")\n",
    "    res2 = weight_update_bounded_divisive(cov_mat,weights,gamma)\n",
    "    res2_expected = np.array([ -0.09819161,  0.09819161  ])\n",
    "    print(f\"expected: {res2_expected} \\t function output {res2}\") \n",
    "    if all(np.isclose(res2,res2_expected,rtol=0.001)):\n",
    "        print(\"**** test PASSED ! ****\")\n",
    "    else:\n",
    "        print(\"**** test FAILED ! ****\")\n",
    "    return\n",
    "\n",
    "def train_weights_vector_field_direction(\n",
    "                          var1,var2,correlation,\n",
    "                          weights,gamma,\n",
    "                          wmin = 0.0, wmax = 1.0,\n",
    "                          do_subtractive=True):\n",
    "    \"\"\"\n",
    "    Computes the vector field of the 2D versions of \n",
    "    weigth_update_bounded_subtractive or weigth_update_bounded_divisive.\n",
    "    The computation is purely numerical... it is the vector difference \n",
    "    between weights after the update and weights before the update.\n",
    "    \"\"\"\n",
    "    if do_subtractive:\n",
    "        weight_update_fun = weight_update_bounded_subtractive\n",
    "    else:\n",
    "        weight_update_fun = weight_update_bounded_divisive\n",
    "    # analytic, for speed\n",
    "    var12 = correlation*np.sqrt(var1*var2)\n",
    "    cov_mat = np.array([[var1,var12],[var12,var2]])\n",
    "    deltaw = weight_update_fun(cov_mat,weights,gamma)\n",
    "    new_weights = weights + deltaw\n",
    "    new_weights[new_weights<wmin] = wmin\n",
    "    new_weights[new_weights>wmax] = wmax\n",
    "    return new_weights - weights\n",
    " \n",
    "    \n",
    "def train_weights_bounded_plot(mean_r1=1.0,mean_r2=3.0,var_r1=1.0,var_r2=1.001,correlation=0.01,\n",
    "                          w1_start=0.3,w2_start=0.5,\n",
    "                          do_subtractive_normalization=True):\n",
    "    wmin,wmax = 0.0,1.0\n",
    "    Tcycle = 60.0\n",
    "    Ncycles = 200\n",
    "    gammahat=1E-3\n",
    "    times,weights = train_weights_bounded(\n",
    "                          mean_r1,var_r1,mean_r2,var_r2,correlation,\n",
    "                          w1_start,w2_start,\n",
    "                          wmin = wmin, wmax = wmax,\n",
    "                          do_subtractive=do_subtractive_normalization,\n",
    "                          Tcycle=Tcycle,Ncycles=Ncycles,gammahat=gammahat)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    ax.scatter(weights[0,:],weights[1,:])\n",
    "    ax.axis(\"equal\")\n",
    "    ax.set_xlim((wmin-0.05,wmax*1.05))\n",
    "    ax.set_ylim((wmin-0.05,wmax*1.05))\n",
    "    ax.set_xlabel(\"weight 1\")\n",
    "    ax.set_ylabel(\"weight 2\")\n",
    "    # now the background\n",
    "    x = np.arange(wmin,wmax*1.01,0.1)\n",
    "    y = np.copy(x)\n",
    "    nw = len(x)\n",
    "\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "\n",
    "    XY = np.stack([X,Y],axis=2)\n",
    "    VXY = np.empty_like(XY)\n",
    "    gamma = gammahat*Tcycle\n",
    "    weights_direction = lambda w : train_weights_vector_field_direction(\n",
    "                          var_r1,var_r2,correlation,\n",
    "                          w,gamma,\n",
    "                          wmin = wmin, wmax = wmax,\n",
    "                          do_subtractive = do_subtractive_normalization)\n",
    "\n",
    "\n",
    "    for ix in range(nw):\n",
    "        for iy in range(nw):\n",
    "            VXY[ix,iy,:] = weights_direction(XY[ix,iy,:])\n",
    "    Vx = VXY[:,:,0]\n",
    "    Vy = VXY[:,:,1]\n",
    "    ax.quiver(X,Y,Vx,Vy, linewidth=None, color=\"black\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize noisy rate inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we consider a one-layer, feedforward rate-based network, where $N$ input neurons are connected to a single oputput neuron. The spiking rates at time $t$ are denoted as $r_j(t)$, with $j=1,2,\\ldots\\,N$. The input rates are generated as a stochastic Ornstein-Uhlenbeck process, with code already provided.\n",
    "\n",
    "In the figure below, you can see the time traces of 2 input neurons, $r_1(t)$ and $r_2(t)$, simulated for 60 seconds. Try to modify some of the parameters to understand their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_r1_and_r2,correlation=(-0.99,0.99,0.01) , mean_r1=(0.0,5.0,0.1),mean_r2=(0.0,5.0,0.1),\n",
    "        var_r1=(0.01,2.0,0.01), var_r2=(0.01,2.0,0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have extra time, check the documentation of the functions `ornstein_uhlenbeck(...)` and `twodimensional_OU(...)`, which are used to generate these traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 . Compute response of output neuron based on input activity and weights\n",
    "\n",
    "The response of the output neruon, $r_\\text{out}(t)$ is the weghted sum of the activities of the input neurons.  \n",
    "\n",
    "We use $w_j$ to represent the weight of the connection that goes from input neuron $j$ to the output neuron. Therefore: \n",
    "$$\n",
    "r_\\text{out}(t) = \\sum_{j=1}^N w_j \\;r_j(t)\n",
    "$$\n",
    "\n",
    "\n",
    "In the exercise below, you should compute $r_\\text{out}(t)$ given the input activities $\\left( r_1(t),\\ldots r_N(t) \\right)$ and the synaptic weights $(w_1,\\ldots,w_N)$. For efficiency, the calculation should be performed on all timesteps simultaneously. In other words, the argument `r_input` is a matrix, with dimensions  $(\\text{neuron index}) \\times (\\text{timestep})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def rate_response(r_input,weights):\n",
    "    \"\"\"\n",
    "    Computes the response of a neuron that receives a series of inputs over time.  \n",
    "    \n",
    "    Parameters :\n",
    "    r_input (2D numpy array) :  r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (1D numpy array) :  weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    \n",
    "    Returns :\n",
    "    r_output (2D numpy array) : r_output[t] is the rate of the output neuron at timestep t\n",
    "    \"\"\"\n",
    "    \n",
    "    # I think this can be done with np.dot , but I don't like to see it applied to matrices\n",
    "    # so I propose a more canonical broadcasting\n",
    "    \n",
    "    r_input_weighted = r_input * weights[:,np.newaxis] # multiply columnwise\n",
    "    r_output = r_input_weighted.sum(axis=0) # and sum columnwise\n",
    "    # r_output[r_output < 0 ] = 0  # avoid negative rates\n",
    "    return r_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to verify that the function behaves as expected.\n",
    "# If you se an error, or the test fails, please revise the exercise \n",
    "test_rate_response()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute weight update for correlation-based and covariance-based rules\n",
    "\n",
    "Now that you have the reponse of the output neuron  $r_\\text{out}(t)$ , you can calculate the update in synaptic weights due to rate-based plasticiy.\n",
    "\n",
    "### Some notation\n",
    "We use $\\left< \\; \\ldots \\; \\right>_t$ to indicate an average over time. For mean rates, we further simplify the notation, taking the form $ \\bar{r}_j$. Therefore:\n",
    "$$\n",
    "\\bar{r}_j = \\left< r_j(t) \\right>_t = \n",
    "\\frac1T \\int_0^T r_j(t) \\;\\mathrm d t = \n",
    "\\frac{1}{N_T} \\sum_k  r_j(t_k)\n",
    "$$\n",
    "The last equality represents the fact that $r(t)$ is discretized in our code, and $N_T$ indicates the number of discretized steps.\n",
    "\n",
    "\n",
    "### Correlation-based rule\n",
    "The correlation-based rule is proportional to the correlation between input and output activity. Formally :\n",
    "$$ \n",
    "\\Delta w_j = \\gamma \\; \\left<  r_\\text{out}(t) \\; r_j(t)  \\right>_t\n",
    "$$\n",
    "Where $\\gamma$ is the learning coefficient, assumed to be a small quantity. The correlation can be computed numerically simply as the mean of the element-wise product between the two time series.\n",
    "$$\n",
    "\\left<  r_\\text{out}(t) \\; r_j(t)  \\right>_t = \\frac{1}{N_T}\\sum_k  r_\\text{out}(t_k) \\; r_j(t_k) \n",
    "$$\n",
    "\n",
    "\n",
    "----\n",
    "*Technical note:*  \n",
    "the weight update rate should really be $\\gamma=\\hat{\\gamma}\\;T$, where $\\hat{\\gamma}$ is the weight update *per second*.  \n",
    "Consider also dimensional analysis: if $\\text{rate}\\sim \\text{time}^{-1}$, then it must be $\\gamma \\sim \\text{time}\\times \\text{weight}$.  \n",
    "\n",
    "---\n",
    "\n",
    "In the next exercise, you should compute $\\left( \\Delta w_1, \\ldots \\Delta w_N \\right)$, given a certain input activity, and some initial weights $(w_1,\\ldots w_N)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_correlation(r_input,weights,gamma):\n",
    "    \"\"\"\n",
    "    Computes a single weight update according to the correlation rule\n",
    "    \n",
    "    Parameters :\n",
    "    r_input (2D numpy array) : r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (1D numpy array) : weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    gamma   (number) : plasticity parameter\n",
    "    T       (number) : total simulation time, in seconds\n",
    "    \n",
    "    Returns :\n",
    "    weight_updates (1D numpy array) : the update on each weight after this training interval\n",
    "    \"\"\"\n",
    "\n",
    "    # TIPS : \n",
    "    # use the rate_response function that you defined before !\n",
    "   \n",
    "    r_output = rate_response(r_input,weights)\n",
    "    r_product  = r_output * r_input  # broadcast by row\n",
    "    weight_updates = gamma * r_product.mean(axis=1) # average over time dimension\n",
    "    return weight_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the exercise\n",
    "\n",
    "To test your function, run the cell below. It should return `test PASSED !` if it errors, or returns `test FAILED !` please review your solution before you proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "test_weight_update_correlation()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Covariance-based rule\n",
    "In the covariance-based rule, we are using a covariance instead of a correlation. The covariance is a \"centered correlation\". That is, we subtract the mean from each trace, before correlating them, as follows:\n",
    "$$ \n",
    "\\Delta w_j = \\gamma \\; \\left<  \\left(r_\\text{out}(t) - \\bar{r}_\\text{out}\\right)\n",
    "\\; \\left(r_j(t) - \\bar{r}_j \\right)  \\right>_t \\quad \\text{with} \\quad \n",
    "\\bar{r}_\\text{out} = \\left< r_\\text{out}(t) \\right>_t \\quad \\text{and} \\quad\n",
    "\\bar{r}_j = \\left< r_j(t) \\right>_t\n",
    "$$\n",
    "\n",
    "In the next exercise, you should compute the weight update in the covariance based case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_covariance(r_input,weights,gamma):\n",
    "    \"\"\"\n",
    "    Computes the weight updates according to the covariance rule\n",
    "    \n",
    "    Parameters :\n",
    "    r_input (2D numpy array) :  r_input[i,t] is the rate of input neuron i at timestep t\n",
    "    weights (1D numpy array) :  weights[i] is the synaptic strenght between neuron i and the output neuron\n",
    "    \n",
    "    Returns :\n",
    "    weight_updates (1D numpy array) : the update on each weight after this training interval\n",
    "    \"\"\"\n",
    "    \n",
    "    # TIPS : \n",
    "    # it is very similar to the correlation rule, except you need to subtract the mean rates!\n",
    "    # r_input_means = r_input.mean(axis=1)  # (mean over time axis)\n",
    "    r_output = rate_response(r_input,weights)\n",
    "    r_output_mean = r_output.mean() # mean of a vector -> scalar value\n",
    "    r_output_meanzero = r_output - r_output_mean\n",
    "    r_input_means = r_input.mean(axis=1) # mean over time axis\n",
    "    r_input_meanzero = r_input - r_input_means[:,np.newaxis] # broadcast on columns\n",
    "    # now same as before\n",
    "    r_product = r_output_meanzero * r_input_meanzero # elementwise product\n",
    "    weight_updates = gamma * r_product.mean(axis=1) # mean over time\n",
    "    return weight_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the exercise\n",
    "\n",
    "As before, to test your function, run the cell below. It should return `test PASSED !` if it errors, or returns `test FAILED !` please review your solution before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST \n",
    "test_weight_update_covariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize $w(t)$ for covariance vs correlation rule\n",
    "\n",
    "In the exercises above you computed a single $\\Delta w_j$, but what happens over a longer training session?\n",
    "\n",
    "For simplicity, we consider a system with two input neurons, and we visualize the evolution of $\\Delta w_1$. We use the following protocol:\n",
    "  1. start with random (and small) initial weights\n",
    "  1. simulate the activity of the external neurons for 60 seconds\n",
    "  1. compute the activity of the output neuron, considering the weights as stationary\n",
    "  1. compute a **single** weight update $(\\Delta w_1,\\Delta w_2)$ from the correlation (or covariance) measured within this interval\n",
    "  1. update and save the current weights\n",
    "  1. go back to point 2. , and iterate for $N_{\\text{iter}}$ times.\n",
    "\n",
    "The code is already provided (based on the functions you completed in the previous exercises). The weight tends to grow exponentially over time, therefore we show it using a logarithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(weight_evo_all, mean_r1=(0.0,5.0,0.1),var1=(0.01,2.0,0.01),\n",
    "         mean_r2=(0.0,5.0,0.1),var2=(0.01,2.0,0.01) , correlation=(-0.99,0.99,0.01));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Why do we say that there is a division between timescales, in this model?\n",
    "+ Which rule results in a faster growth of the weight, and why?\n",
    "+ Which factor contributes the most in the correlation-based rule ? What about the covariance-based rule?\n",
    "+ When do the two rules coincide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The covariance rule corresponds to the first principal component of the input rates\n",
    "\n",
    "Let's consider again the time-traces of two input neurons, $r_1(t)$ and $r_2(t)$. This time please take a look at the second plot: it shows samples of the two activities over time.\n",
    "\n",
    "The **first principal component**, showna as a red line, represents the direction where most of the variance lies. \n",
    "\n",
    "Here we use `scikit-learn` to compute it: \n",
    "```python\n",
    "r_input = ... <generate neural activity>\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(r_input)\n",
    "first_component = pca.components_[0,:]\n",
    "```\n",
    "Warning ! For the scikit-learn fit we need the format `r_input[k,i]`, where `k` represents time `t[k]` and `i` is the neuron. The function has already been implemented, but here you can change the input parameters and see how the PCA direction adapts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_r1_and_r2_with_PCA,correlation=(-0.99,0.99,0.01) , mean_r1=(0.0,5.0,0.1),mean_r2=(0.0,5.0,0.1),\n",
    "        var_r1=(0.01,2.0,0.01), var_r2=(0.01,2.0,0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the direction of the PCA with the normalized weights $(w_1,w_2)$ after learning with the covariance rule. The two directions are very similar.\n",
    "\n",
    "(once again, the code is already provided, but you can check the function `plot_covrule_and_PCA(...)` to see how it is done )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(plot_covrule_and_PCA,correlation=(-0.99,0.99,0.01) , mean_r1=(0.0,5.0,0.1),mean_r2=(0.0,5.0,0.1),\n",
    "        var_r1=(0.01,2.0,0.01), var_r2=(0.01,2.0,0.01));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounded plasticity\n",
    "In this section, we modify the plasticity rule, so that synaptic weights do not diverge to $+\\infty$ exponentially. In this part, we will be using a matrix and vector notation. First of all, we denote the covariance matrix of the input rates as $C$. Therefore :\n",
    "$$\n",
    "C_{ij} := \\left<  \\left(r_i(t) - \\bar{r}_i\\right)\n",
    "\\; \\left(r_j(t) - \\bar{r}_j \\right)  \\right>_t\n",
    "$$\n",
    "\n",
    "We consider two types of normalization, defined in vector form:\n",
    "\n",
    "**Subtractive normalization**\n",
    "$$\n",
    "\\Delta \\mathbf w = \\gamma \\; \\left(  C \\, \\mathbf w -\n",
    "\\frac{\\boldsymbol{1}^\\top \\,C \\,\\mathbf{w}}{N} \\boldsymbol{1}  \\right)\n",
    "\\;\\; \\text{where}\\;\\; \\mathbf w = (w_1,w_2,\\ldots w_N)\n",
    "\\;\\; \\text{and}\\;\\; \\boldsymbol{1} = (1,1\\ldots 1)  \\;\\;\\text{$N$ times}\n",
    "$$\n",
    "**Divisive normalization**\n",
    "$$\n",
    "\\Delta \\mathbf w = \\gamma \\; \\left(  C \\, \\mathbf w -\n",
    "\\frac{\\boldsymbol{1}^\\top \\,C \\,\\mathbf{w}}{ \\boldsymbol{1}^\\top \\,\\mathbf{w}}\n",
    "\\mathbf{w}  \\right)\n",
    "\\;\\; \\text{where}\\;\\; \\mathbf w = (w_1,w_2,\\ldots w_N)\n",
    "\\;\\; \\text{and}\\;\\; \\boldsymbol{1} = (1,1\\ldots 1)  \\;\\;\\text{$N$ times}\n",
    "$$\n",
    "\n",
    "The last exercise consists in computing a single weight update for these two new rules, given the covariance matrix $C$ and the initial weights $\\mathbf w = (w_1,w_2,\\ldots w_N) $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_bounded_subtractive(cov_mat,weights,gamma):\n",
    "    \"\"\"\n",
    "    Returns the covariance-based weight update, using subtractive normalization \n",
    "    \n",
    "    Parameters :\n",
    "    cov_mat (2D numpy array) : covariance matrix of the input activity\n",
    "    weights   (1D numpy array) : current input weights\n",
    "    gamma   (number) : learning coefficient \n",
    "    \n",
    "    Returns :\n",
    "    delta_weights (1D numpy array) : update to add on each weight from the plasticity rule\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    onevec = np.ones(N)\n",
    "    cov_times_w = cov_mat @ weights\n",
    "    return gamma * ( cov_times_w - cov_times_w.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "def weight_update_bounded_divisive(cov_mat,weights,gamma):\n",
    "    \"\"\"\n",
    "    Returns the covariance-based weight update, using divisive normalization \n",
    "    \n",
    "    Parameters :\n",
    "    cov_mat (2D numpy array) : covariance matrix of the input activity\n",
    "    weights   (1D numpy array) : current input weights\n",
    "    gamma   (number) : learning coefficient \n",
    "    \n",
    "    Returns :\n",
    "    delta_weights (1D numpy array) : update to add on each weight from the plasticity rule\n",
    "    \"\"\"\n",
    "    N = len(weights)\n",
    "    onevec = np.ones(N)\n",
    "    cov_times_w = cov_mat @ weights\n",
    "    wsum = weights.sum()\n",
    "    if wsum == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return gamma * ( cov_times_w - (cov_times_w.sum()/wsum) * weights )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test the function and validate the exercise. \n",
    "# WARNING : If the test fails, the next blocks will error too!\n",
    "test_weight_update_bounded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train weights with bounded plasticity\n",
    "\n",
    "The next exercise is to complete the missing parts in the training function. The algorithm is similar to the training you encountered in the unbounded case, but now you will complete parts of the code! \n",
    "\n",
    "The algorithm does the following:\n",
    "   1. simulate input neural rates for 60 seconds\n",
    "   1. compute the covariance matrix\n",
    "   1. update the weights, save them\n",
    "   1. go back to point 1, until convergence (e.g. a sufficiently long time)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_weights_bounded(mean_r1,var_r1,mean_r2,var_r2,correlation,\n",
    "                          w1_start,w2_start,\n",
    "                          wmin = 0.0, wmax = 1.0,\n",
    "                          do_subtractive=True,\n",
    "                          Tcycle=60.0,Ncycles=100,gammahat=1E-3):\n",
    "    \"\"\"\n",
    "    Training of synaptic weights with two input neurons, using bounded, \n",
    "    covariance-based weight updates.\n",
    "    \n",
    "    Parameters :\n",
    "    mean_r1 (number) : mean of input neuron 1\n",
    "    var_r1 (number) :  variance of input neuron 1\n",
    "    mean_r2 (number) :  -\n",
    "    var_r2 (number) :   -\n",
    "    correlation (number) : correlation between input neurons 1 and 2\n",
    "    w1_start (number) : initial weight from neuron 1 to output neuron\n",
    "    w2_start (number) : initial weight from neuron 2 to output neuron\n",
    "    wmin = 0.0 : minimum weight (hard bound)\n",
    "    wmax = 1.0 : maximum weight (hard bound)\n",
    "    do_subtractive = True : if tue, does subtractive normalization, if false does divisive normalization \n",
    "    Tcycle = 60.0 : duration of neural rates simulations (one sessions) \n",
    "    Ncycles = 100 : number of sessions (i.e. number of weight updates)\n",
    "    gammahat = 0.001 : learning coefficient\n",
    "    \n",
    "    Returns :\n",
    "    times_ret (1D numpy array) : times of weight updates\n",
    "    weights (2D numpy array) : weights[i,k] is the weight associated to neuron i at time times_ret[k] \n",
    "    \"\"\"\n",
    "    N = 2\n",
    "    \n",
    "    # select which normalization takes place\n",
    "    # (E) : complete with function names\n",
    "    if do_subtractive:\n",
    "        weight_update_fun = weight_update_bounded_subtractive\n",
    "    else:\n",
    "        weight_update_fun = weight_update_bounded_divisive\n",
    "    \n",
    "    # these are the times at which the weights are updated\n",
    "    times_ret = np.arange(0,Tcycle*Ncycles,Tcycle)\n",
    "    \n",
    "    weights = np.empty((N,Ncycles))\n",
    "    weights[:,0] = np.array([w1_start,w2_start]) \n",
    "    \n",
    "    # Do several 60 second cycles\n",
    "    gamma = gammahat * Tcycle\n",
    "    for k in range(Ncycles-1):\n",
    "        _time,_rate_inputs = twodimensional_OU(mean_r1,var_r1,mean_r2,var_r2,correlation,1.0,Tcycle)\n",
    "        # (E) compute the covariance matrix of the input rates using np.cov(...)\n",
    "        # then update the weights using the functions defined before\n",
    "        _cov_mat = np.cov(_rate_inputs)\n",
    "        wtemp =  weights[:,k] + weight_update_fun(_cov_mat,weights[:,k],gamma)\n",
    "        # (E) impose hard boundaries on the result, so that it is between wmin and wmax\n",
    "        wtemp[wtemp<wmin] = wmin\n",
    "        wtemp[wtemp>wmax] = wmax\n",
    "        weights[:,k+1] = wtemp\n",
    "    \n",
    "    return times_ret, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the code interactively \n",
    "\n",
    "The script below will test the function, for different parameters.\n",
    "\n",
    "Use the checkbox at the bottom to switch between subtractive and multiplicative normalization, and press \"Run Interact\" to see the result. The code is not optimized for speed, so it might take 10 to 20  seconds before the result appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(train_weights_bounded_plot,mean_r1=(0.0,5.0,0.1),mean_r2=(0.0,5.0,0.1),\n",
    "        var_r1=(0.01,5.0,0.01), var_r2=(0.01,5.0,0.01),correlation=(-0.99,0.99,0.01),\n",
    "        w1_start = (0.01,1.0,0.01) ,w2_start = (0.01,1.0,0.01)   );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : analytic exercise\n",
    "\n",
    "You might have notived that in the first part we expressed the covariance-based rule as:\n",
    "$$ \n",
    "\\Delta w_j = \\gamma \\; \\left<  \\left(r_\\text{out}(t) - \\bar{r}_\\text{out}\\right)\n",
    "\\; \\left(r_j(t) - \\bar{r}_j \\right)  \\right>_t \\quad\n",
    "\\text{with} \\quad\n",
    "r_\\text{out}(t) = \\sum_{j=1}^N w_j \\,r_j(t) \n",
    "$$\n",
    "\n",
    "However in the second part we used the equivalent form (vectorized):\n",
    "$$\n",
    "\\Delta \\mathbf w = \\gamma \\; C \\, \\mathbf w  \\quad\n",
    "\\text{with} \\quad\n",
    "C_{i,j} = \\left<  \\left(r_i(t) - \\bar{r}_i\\right)\n",
    "\\; \\left(r_j(t) - \\bar{r}_j \\right)  \\right>_t \n",
    "$$\n",
    "\n",
    "Prove analytically that the two formulations are equivalent.\n",
    "\n",
    "\n",
    "Solution (remove this in version for students):\n",
    "\n",
    "https://github.com/comp-neural-circuits/plasticity-workshop/raw/dev/covariance_rule_exercise.pdf"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5e3b54db-e48f-43a7-b40f-09467f2d2c99",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
